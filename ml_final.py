# -*- coding: utf-8 -*-
"""ML_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_dZpQyH0k_SeL95aCx5hw9Xvn6eAS4u

##Импорт библиотек
"""

pip install category_encoders

!pip install -q phik

pip install catboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from category_encoders import CatBoostEncoder
from scipy import stats
from sklearn.preprocessing import PowerTransformer
import phik
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.model_selection import train_test_split
from catboost import Pool, cv, CatBoostClassifier
from IPython.core.debugger import set_trace
import joblib

"""##Импорт датасетов"""

train_df = pd.read_csv('df_train.csv')
test_df = pd.read_csv('df_test.csv')
train_df['target'].value_counts()

"""Проверка на отсутствующие значения"""

train_df.dropna()
train_df.describe()

"""Преобразем CreatedDate в float (количество секунд с начала эпохи)


"""

#Преобразем CreatedDate в float (количество секунд с начала эпохи)
train_df_date = train_df
train_df_date['CreatedDate'] = pd.to_datetime(train_df_date['CreatedDate'], utc=True)
train_df_date['CreatedDate'] = train_df_date['CreatedDate'].astype('int64')/ 1e9

#для категориальной информации используем Catboost
cat_cols = ['service', 'PaymentType']
encoder = CatBoostEncoder(cols=cat_cols)
df_encoded = encoder.fit_transform(train_df_date[cat_cols], train_df_date['target'])
train_df_nocat = train_df_date.drop(columns=cat_cols)
train_df_nocat = pd.concat([train_df_nocat, df_encoded], axis=1)
train_df_nocat.describe()

sns.boxplot(y = 'PaymentType', hue = 'target', data=train_df_nocat, orient = 'v')

"""Распределения данных"""

#Рассмотрим распределения данных
train_df_nocat.hist(figsize=(20, 10), layout=(-1, 5))
plt.show()

def antitrash(df, coef):
    df_antitrash = df[(np.abs(stats.zscore(df)) < coef).all(axis=1)]
    return(df_antitrash)

train_df_nocat['IsPaid'] = train_df_nocat['IsPaid'].astype(int)
train_df_nocat_antitrash = antitrash(df = train_df_nocat, coef = 4)
train_df_nocat_antitrash.describe()
#train_df_nocat_antitrash.hist(figsize=(20, 10), layout=(-1, 5))
#plt.show()

def remove_rows_with_custom_thresholds(df, threshold_dict):
    mask = pd.Series(True, index=df.index)
    for column, threshold in threshold_dict.items():
        mask &= df[column] <= threshold
    return df[mask]

thresholds = {
    'unique_items': 500,
    'number_of_orders': 180,
    'number_of_ordered_items': 1500
}

filtered_df = remove_rows_with_custom_thresholds(train_df_nocat_antitrash, thresholds)

filtered_df.hist(figsize=(20, 10), layout=(-1, 5))
plt.show()

columns_to_transform = ['user_id', 'nm_id', 'total_ordered', 'count_items', 'unique_items', 'avg_unique_purchase','NmAge','Distance','DaysAfterRegistration', 'number_of_orders', 'number_of_ordered_items', 'mean_number_of_ordered_items', 'min_number_of_ordered_items', 'max_number_of_ordered_items', 'mean_percent_of_ordered_items', 'CreatedDate']
# Инициализируем PowerTransformer с методом 'yeo-johnson'
train_df_nocat_yj = train_df_nocat_antitrash.copy()
pt = PowerTransformer(method='yeo-johnson', standardize=True)

transformed_data = pd.DataFrame(
    pt.fit_transform(train_df_nocat_antitrash[columns_to_transform]),
    columns=columns_to_transform,
    index=train_df_nocat_antitrash.index
)

train_df_nocat_yj.update(transformed_data)
train_df_nocat_yj.hist(figsize = (20, 20), layout = (-1, 5))

train_df_nocat_yj.describe()

def phiq_corr_mat(df):
    plt.figure(figsize=(9, 9))

    sorted_columns = (
        df.phik_matrix(interval_cols=df.columns)
        .round(2)
        .sort_values("target", ascending=False, axis=1)
        .columns
    )

    heatmap = sns.heatmap(
        df.phik_matrix(interval_cols=df.columns)
        .round(2)
        .sort_values("target", ascending=False, axis=1)
        .reindex(sorted_columns),
        annot=True,
        square=True,
        cmap="Blues",
        vmax=1,
        vmin=0,
        cbar_kws={"fraction": 0.01},  # shrink colour bar
        linewidth=2,
    )

    heatmap.set_xticklabels(
        heatmap.get_xticklabels(), rotation=45, horizontalalignment="right"
    )
    heatmap.set_title("Phik heatmap sorted by target", fontdict={"fontsize": 18}, pad=16)
    plt.show()
phiq_corr_mat(train_df_nocat_yj)

sns.pairplot(filtered_df)

"""# Решение проблемы дисбаланса классов"""

from imblearn.combine import SMOTEENN

X = filtered_df.drop('target', axis = 1)
y = filtered_df['target']
sme = SMOTEENN(random_state=42)
X_res, y_res = sme.fit_resample(X, y)

X_res.describe()

y_res.value_counts()

"""## Подбор модели, её гиперпараметров и признаков"""

test_df_date = test_df
test_df_date['CreatedDate'] = pd.to_datetime(test_df['CreatedDate'], utc=True)
test_df_date['CreatedDate'] = test_df_date['CreatedDate'].astype('int64')/ 1e9

test_df_labels = test_df_date['target']
test_df_features = test_df_date.drop('target', axis = 1)

def cat_encoding_test(df, cat_cols):
    test_features_nocat = encoder.transform(df[cat_cols])
    df = df.drop(columns = cat_cols)
    test_features_nocat = pd.concat([df, test_features_nocat], axis = 1)
    return(test_features_nocat)
cat_cols = ['service', 'PaymentType']
test_features_nocat = cat_encoding_test(test_df_features, cat_cols)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report

train_val_labels = y_res
#train_val_data = X_res
train_val_data = X_res.drop(['user_id', 'CreatedDate', 'PaymentType', 'is_courier', 'IsPaid', 'service', 'number_of_ordered_items', 'nm_id', 'max_number_of_ordered_items', 'min_number_of_ordered_items', 'mean_number_of_ordered_items', 'total_ordered', 'number_of_orders', 'unique_items', 'Distance'], axis = 1)
X_train, X_val, y_train, y_val = train_test_split(train_val_data, train_val_labels, test_size = 0.2, random_state = 15, shuffle=True)

test_features = test_features_nocat.drop(['user_id', 'CreatedDate', 'PaymentType', 'is_courier', 'IsPaid', 'service', 'number_of_ordered_items', 'nm_id', 'max_number_of_ordered_items', 'min_number_of_ordered_items', 'mean_number_of_ordered_items', 'total_ordered', 'number_of_orders', 'unique_items', 'Distance'], axis = 1)
test_df_labels = test_df_date['target']

catboost = CatBoostClassifier(verbose=0, random_state=42, early_stopping_rounds=10, depth = 4, l2_leaf_reg = 3, learning_rate = 0.03, iterations = 200)
catboost.fit(X_train, y_train)

joblib.dump(catboost, 'catboost_model.joblib')

catboost.set_probability_threshold(binclass_probability_threshold=0.6)

y_pred_test = catboost.predict(test_features)
y_proba_test = catboost.predict_proba(test_features)[:, 1]  # для ROC AUC

print("Accuracy:", accuracy_score(test_df_labels, y_pred_test))
print("Precision:", precision_score(test_df_labels, y_pred_test))
print("Recall:", recall_score(test_df_labels, y_pred_test))
print("F1 Score:", f1_score(test_df_labels, y_pred_test))
print("ROC AUC:", roc_auc_score(test_df_labels, y_proba_test))
print("Confusion Matrix:\n", confusion_matrix(test_df_labels, y_pred_test))
print("\nClassification Report:\n", classification_report(test_df_labels, y_pred_test))

y_pred_val = catboost.predict(X_val)
y_proba_val = catboost.predict_proba(X_val)[:, 1]  # для ROC AUC
print("\nClassification Report:\n", classification_report(y_val, y_pred_val))

import sys
catboost.get_feature_importance(data=Pool(data = X_res, label = y_res),
                       reference_data=None,
                       #type=EFstrType.FeatureImportance,
                       prettified=True,
                       thread_count=-1,
                       verbose=True,
                       log_cout=sys.stdout,
                       log_cerr=sys.stderr
                        )

"""#Пробуем Логистическую регрессию в качестве классификатора"""

from sklearn.linear_model import LogisticRegression

param_grid = {
    'C': [0.01, 0.05, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['newton-cholesky']
}

grid = GridSearchCV(LogisticRegression(max_iter=2000, class_weight = 'balanced'), param_grid, cv=5, scoring='f1_weighted')
grid.fit(train_val_data, train_val_labels)

# 9. Предсказания и метрики
y_pred = grid.predict(test_features)
y_proba = grid.predict_proba(test_features)[:, 1]

print("Лучшие параметры:", grid.best_params_)
print("\nClassification Report:\n", classification_report(test_df_labels, y_pred))
print("Confusion Matrix:\n", confusion_matrix(test_df_labels, y_pred))
print("ROC AUC Score:", roc_auc_score(test_df_labels, y_proba))

model = grid.best_estimator_

coefs = pd.Series(model.coef_[0])

print("\nТоп-10 важных признаков:")
print(coefs.head(10))

# === 5. Классификация с пользовательским порогом ===
y_proba = model.predict_proba(X_test_all)[:, 1]
custom_threshold = 0.3
y_pred_custom = (y_proba >= custom_threshold).astype(int)

print(f"\nМетрики при пороге = {custom_threshold}")
print("Classification Report:\n", classification_report(y_test, y_pred_custom))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_custom))
print("ROC AUC Score:", roc_auc_score(y_test, y_proba))

best_model_Lin.get_feature_importance(data=Pool(data = train_val_data, label = train_val_labels),
                       reference_data=None,
                       #type=EFstrType.FeatureImportance,
                       prettified=True,
                       thread_count=-1,
                       verbose=True,
                       log_cout=sys.stdout,
                       log_cerr=sys.stderr
                        )

"""#Пробуем сделать через встроенные праметры Catboost Classifier"""

X_train_val= train_df_nocat.drop('target', axis = 1)
y_train_val = train_df_nocat['target']
scaler = StandardScaler()
scaler.fit(X)
train_val_data_for_weights = pd.DataFrame(scaler.transform(X), columns = X.columns)

train_val_data_for_weights

len(y_train_val[y_train_val == 0]) / len(y_train_val[y_train_val == 1])

def cat_encoding_test(df, cat_cols):
    test_features_nocat = encoder.transform(df[cat_cols])
    df = df.drop(columns = cat_cols)
    test_features_nocat = pd.concat([df, test_features_nocat], axis = 1)
    return(test_features_nocat)
cat_cols = ['service', 'PaymentType']

test_features_nocat = cat_encoding_test(test_df_features, cat_cols)
test_features_standard = pd.DataFrame(scaler.transform(test_features_nocat), columns = test_features_nocat.columns)

test_features_nocat

test_df_labels = test_df_labels
test_df_features = test_features_standard

from catboost import CatBoostClassifier
from sklearn.model_selection import GridSearchCV

model = CatBoostClassifier(verbose = 0,
                           scale_pos_weight = len(y_train_val[y_train_val == 0]) / len(y_train_val[y_train_val == 1]))  # убираем лишний вывод

param_grid = {
    'depth': [4, 6, 8],
    'iterations': [100, 200],
    'learning_rate': [0.01, 0.1],
    'l2_leaf_reg': [1, 3, 5],
    'border_count': [32, 64]
}

grid = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=3,
    scoring='f1_weighted',
    n_jobs=-1
)

grid.fit(train_val_data_for_weights, y_train_val)

print("Best score:", grid.best_score_)
print("Best params:", grid.best_params_)

best_model = grid.best_estimator_

y_pred = best_model.predict(test_df_features)
y_proba = best_model.predict_proba(test_df_features)[:, 1]  # для ROC AUC

print("Accuracy:", accuracy_score(test_df_labels, y_pred))
print("Precision:", precision_score(test_df_labels, y_pred))
print("Recall:", recall_score(test_df_labels, y_pred))
print("F1 Score:", f1_score(test_df_labels, y_pred))
print("ROC AUC:", roc_auc_score(test_df_labels, y_proba))
print("Confusion Matrix:\n", confusion_matrix(test_df_labels, y_pred))
print("\nClassification Report:\n", classification_report(test_df_labels, y_pred))